{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Duet Neural Document Ranking Model\n",
    "\n",
    "This is a tutorial for implementing the Duet neural document ranking model using the [Microsoft Cognitive Tookit](https://www.microsoft.com/en-us/cognitive-toolkit/) as described in the following papers:\n",
    "\n",
    "1. [Learning to Match Using Local and Distributed Representations of Text for Web Search](https://arxiv.org/abs/1610.08136). Bhaskar Mitra, Fernando Diaz, and Nick Craswell. In Proceedings of the 26th International Conference on World Wide Web, WWW â€“ World Wide Web Consortium (W3C). April, 2017.\n",
    "\n",
    "2. [Benchmark for Complex Answer Retrieval](https://arxiv.org/abs/1705.04803). Federico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. In Proceedings of the 3rd International Conference on the Theory of Information Retrieval (ICTIR), ACM. October, 2017.\n",
    "\n",
    "Duet is a document ranking model composed of two separate deep neural networks, one that matches the query and the document using a _local representation_, and another that matches the query and the document using learned [_distributed representations_](http://web.stanford.edu/class/psych209a/ReadingsByDate/02_01/HintonMcCRumelhart86DistribRep.pdf). The two networks are jointly trained as part of a single neural network.\n",
    "\n",
    "**Note**: The Bing document ranking dataset used for training/evaluating the duet model in the [original paper](https://arxiv.org/abs/1610.08136) is a proprietary dataset that I **can not** share because of privacy considerations. However, the [TREC CAR dataset](http://trec-car.cs.unh.edu/) used in the [following paper](https://arxiv.org/abs/1705.04803) _is_ publicly available that you may want to consider if you are looking for a large labelled dataset for DNN benchmarking.\n",
    "\n",
    "\n",
    "### Change log\n",
    "**Jan 25, 2017**: The original Duet model was implemented on CNTK using [BrainScript](https://docs.microsoft.com/en-us/cognitive-toolkit/Using-CNTK-with-BrainScript) and [a custom CNTK reader](https://github.com/Microsoft/CNTK/tree/bmitra/NDRMReader/Source/Readers/NDRMReader). I have re-implemented the original model on CNTK v2 using the new [python API](https://www.cntk.ai/pythondocs/).\n",
    "\n",
    "**Nov 07, 2017**: Updated the script for [CNTK v2.2](https://docs.microsoft.com/en-us/cognitive-toolkit/releasenotes/cntk_2_2_release_notes).\n",
    "\n",
    "## Let's start\n",
    "\n",
    "We assume you have already installed CNTK on your machine following the steps outlined [here](https://docs.microsoft.com/en-us/cognitive-toolkit/Setup-CNTK-on-your-machine). So we start begin by importing all the relevant modules that we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import cntk as C\n",
    "\n",
    "\n",
    "C.try_set_default_device(C.cpu(), acquire_device_lock=False)\n",
    "C.cntk_py.set_fixed_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `set_default_device` method we can set the code to run on either CPU or GPU.\n",
    "\n",
    "## Data reader \n",
    "Our Duet implementation accepts data in the format of a tab-separated text file, where the first few columns (optionally) contains some meta-information about each sample (e.g., query or document ID), followed by a column containing query text, and finally some fixed number of columns containing (body) text from the different candidate documents to be ranked for that query. \n",
    "\n",
    "For example, a training data file may look like the following,\n",
    "\n",
    "<table>\n",
    "    <tr><td>some query</td></td><td>relevant document body text</td><td>non-relevant document body text</td></tr>\n",
    "    <tr><td>another query</td></td><td>another relevant document body text</td><td>another non-relevant document body text</td></tr>\n",
    "    <tr><td>yet another query</td></td><td>yet another relevant document body text</td><td>yet another non-relevant document body text</td></tr>\n",
    "</table>\n",
    "\n",
    "At test time, we will need to deal with a variable number of documents per query. So it may be easier to put a single query-document pair per line along with corresponding query / document IDs. For example,\n",
    "\n",
    "<table>\n",
    "    <tr><td>first query ID</td><td>ID for document A</td><td>rating for document A</td><td>first query</td></td><td>document A body text</td></tr>\n",
    "    <tr><td>first query ID</td><td>ID for document B</td><td>rating for document B</td><td>first query</td></td><td>document B body text</td></tr>\n",
    "    <tr><td>first query ID</td><td>ID for document C</td><td>rating for document C</td><td>first query</td></td><td>document C body text</td></tr>\n",
    "    <tr><td>second query ID</td><td>ID for document D</td><td>rating for document D</td><td>second query</td></td><td>document D body text</td></tr>\n",
    "    <tr><td>second query ID</td><td>ID for document E</td><td>rating for document E</td><td>second query</td></td><td>document E body text</td></tr>\n",
    "</table>\n",
    "\n",
    "Note that the `DataReader` below returns input features for both the _local_ and the _distributed_ subnetworks of the Duet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.query = \"\"\n",
    "        self.docs = []\n",
    "    \n",
    "class DataReader:\n",
    "    max_query_words = 10\n",
    "    max_doc_words = 1000\n",
    "    \n",
    "    def __init__(self, data_file, ngraphs_file, num_docs, num_meta_cols, multi_pass):\n",
    "        self.__load_ngraphs(ngraphs_file)\n",
    "        self.data_file = open(data_file, mode='r')\n",
    "        self.num_docs = num_docs\n",
    "        self.num_meta_cols = num_meta_cols\n",
    "        self.multi_pass = multi_pass\n",
    "    \n",
    "    def __load_ngraphs(self, filename):\n",
    "        self.ngraphs = {}\n",
    "        self.max_ngraph_len = 0\n",
    "        with open(filename, mode='r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                self.ngraphs[row[0]] = int(row[1]) - 1\n",
    "                self.max_ngraph_len = max(self.max_ngraph_len, len(row[0]))\n",
    "        self.num_ngraphs = len(self.ngraphs)\n",
    "\n",
    "    def __read_samples(self, num_samples):\n",
    "        labels = np.zeros((num_samples, self.num_docs), dtype=np.float32)\n",
    "        samples = []\n",
    "        meta = []\n",
    "        mb_size = 0\n",
    "        for i in range(num_samples):\n",
    "            row = self.data_file.readline()\n",
    "            if row == \"\":\n",
    "                if self.multi_pass:\n",
    "                    self.data_file.seek(0)\n",
    "                    row = self.data_file.readline()\n",
    "                else:\n",
    "                    break\n",
    "            cols = row.split('\\t')\n",
    "            curr_sample = Sample()\n",
    "            curr_sample.query = re.sub('[^0-9a-z\\t]+', ' ', cols[self.num_meta_cols].lower()).strip()\n",
    "            for j in range(self.num_meta_cols+1, min(self.num_meta_cols+self.num_docs+1, len(cols))):\n",
    "                curr_sample.docs.append(re.sub('[^0-9a-z\\t]+', ' ', cols[j].lower()).strip())\n",
    "            samples.append(curr_sample)\n",
    "            labels[i][0] = np.float32(1)\n",
    "            meta.append([cols[i] for i in range(0, self.num_meta_cols)])\n",
    "            mb_size += 1\n",
    "        return samples, labels, meta, mb_size\n",
    "        \n",
    "    def __get_interaction_features(self, samples):\n",
    "        features = np.zeros((len(samples), self.num_docs, self.max_query_words, self.max_doc_words), dtype=np.float32)\n",
    "        for sample_idx, sample in enumerate(samples):\n",
    "            for doc_idx, doc in enumerate(sample.docs):\n",
    "                for qw_idx, qword in enumerate(sample.query.split()):\n",
    "                    if qw_idx == self.max_query_words:\n",
    "                        break\n",
    "                    for dw_idx, dword in enumerate(doc.split()):\n",
    "                        if dw_idx == self.max_doc_words:\n",
    "                            break\n",
    "                        if qword == dword:\n",
    "                            features[sample_idx, doc_idx, qw_idx, dw_idx] = np.float32(1)\n",
    "        return features\n",
    "        \n",
    "    def __get_ngraph_features(self, samples):\n",
    "        features_query = np.zeros((len(samples), self.num_ngraphs, self.max_query_words), dtype=np.float32)\n",
    "        features_docs = np.zeros((len(samples), self.num_docs, self.num_ngraphs, self.max_doc_words), dtype=np.float32)\n",
    "        for sample_idx, sample in enumerate(samples):\n",
    "            # loop over query and docs -- doc_idx = 0 corresponds to query \n",
    "            for doc_idx in range(len(sample.docs)+1):\n",
    "                doc = sample.query if doc_idx == 0 else sample.docs[doc_idx-1]\n",
    "                max_words = self.max_query_words if doc_idx == 0 else self.max_doc_words\n",
    "                for w_idx, word in enumerate(doc.split()):\n",
    "                    if w_idx == max_words:\n",
    "                        break\n",
    "                    token = '#' + word + '#'\n",
    "                    token_len = len(token)\n",
    "                    for i in range(token_len):\n",
    "                        for j in range(0, self.max_ngraph_len):\n",
    "                            if i+j < token_len:\n",
    "                                ngraph_idx = self.ngraphs.get(token[i:i+j])\n",
    "                                if ngraph_idx != None:\n",
    "                                    if doc_idx == 0:\n",
    "                                        features_query[sample_idx, ngraph_idx, w_idx] += 1\n",
    "                                    else:\n",
    "                                        features_docs[sample_idx, doc_idx-1, ngraph_idx, w_idx] += 1\n",
    "        return features_query, features_docs\n",
    "\n",
    "    def get_minibatch(self, num_samples):\n",
    "        samples, labels, meta, mb_size = self.__read_samples(num_samples)\n",
    "        features_local = self.__get_interaction_features(samples)\n",
    "        features_distrib_query, features_distrib_docs = self.__get_ngraph_features(samples)\n",
    "        return features_local, features_distrib_query, features_distrib_docs, labels, meta, mb_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "\n",
    "The figure below provides a detailed schematic view of the duet architecture. The distributed model projects the query and the document text into an embedding space before matching, while the local model operates over an interaction matrix comparing every query term to every document term. The final score under the duet setup is the sum of scores from the local and the distributed networks,\n",
    "\n",
    "$$f(Q, D) = f_l(Q, D) + f_d(Q, D)$$\n",
    "\n",
    "Where both the query and the document are considered as ordered list of terms. Each query term q and document term d is a m X 1 vector where m is the input representation of the text (e.g. the number of terms in the vocabulary for the local model).\n",
    "\n",
    "![Duet architecture](img/duet-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def duet(features_local, features_distrib_query, features_distrib_docs, num_ngraphs, words_per_query, words_per_doc, num_docs):\n",
    "    num_hidden_nodes = 300\n",
    "    word_window_size = 3\n",
    "    pooling_kernel_width_query = words_per_query - word_window_size + 1 # = 8\n",
    "    pooling_kernel_width_doc = 100\n",
    "    num_pooling_windows_doc = ((words_per_doc - word_window_size + 1) - pooling_kernel_width_doc) + 1 # = 899\n",
    "                        \n",
    "    duet_local    = C.layers.Sequential ([\n",
    "                        C.layers.Convolution((1, words_per_doc), num_hidden_nodes, activation=C.tanh, strides=(1, 1), pad=False),\n",
    "                        C.layers.Dense(num_hidden_nodes, activation=C.tanh),\n",
    "                        C.layers.Dense(num_hidden_nodes, activation=C.tanh),\n",
    "                        C.layers.Dropout(0.2),\n",
    "                        C.layers.Dense(1, activation=C.tanh)])\n",
    "                        \n",
    "    duet_embed_q  = C.layers.Sequential ([\n",
    "                        C.layers.Convolution((word_window_size, 1), num_hidden_nodes, activation=C.tanh, strides=(1, 1), pad=False),\n",
    "                        C.layers.MaxPooling((pooling_kernel_width_query, 1), strides=(1, 1), pad=False),\n",
    "                        C.layers.Dense(num_hidden_nodes, activation=C.tanh)])\n",
    "                        \n",
    "    duet_embed_d  = C.layers.Sequential ([\n",
    "                        C.layers.Convolution((word_window_size, 1), num_hidden_nodes, activation=C.tanh, strides=(1, 1), pad=False),\n",
    "                        C.layers.MaxPooling((pooling_kernel_width_doc, 1), strides=(1, 1), pad=False),\n",
    "                        C.layers.Convolution((1, 1), num_hidden_nodes, activation=C.tanh, strides=(1, 1), pad=False)])\n",
    "                        \n",
    "    duet_distrib  = C.layers.Sequential ([\n",
    "                        C.layers.Dense(num_hidden_nodes, activation=C.tanh),\n",
    "                        C.layers.Dense(num_hidden_nodes, activation=C.tanh),\n",
    "                        C.layers.Dropout(0.2),\n",
    "                        C.layers.Dense(1, activation=C.tanh)])\n",
    "    \n",
    "    net_local       = [C.slice(features_local, 0, idx, idx+1) for idx in range(0, num_docs)]\n",
    "    net_local       = [C.reshape(d, (1, words_per_query, words_per_doc)) for d in net_local]\n",
    "    net_local       = [duet_local(d) for d in net_local]\n",
    "    net_local       = [C.reshape(d, (1, 1)) for d in net_local]\n",
    "    net_local       = C.splice(*net_local)\n",
    "    \n",
    "    net_distrib_q   = C.reshape(features_distrib_query, (num_ngraphs, words_per_query, 1))\n",
    "    net_distrib_q   = duet_embed_q(net_distrib_q)\n",
    "    net_distrib_q   = [net_distrib_q for idx in range(0, num_pooling_windows_doc)]\n",
    "    net_distrib_q   = C.splice(*net_distrib_q)\n",
    "    net_distrib_q   = C.reshape(net_distrib_q, (num_hidden_nodes * num_pooling_windows_doc, 1))\n",
    "    \n",
    "    net_distrib_d   = [C.slice(features_distrib_docs, 0, idx, idx+1) for idx in range(0, num_docs)]\n",
    "    net_distrib_d   = [C.reshape(d, (num_ngraphs, words_per_doc, 1)) for d in net_distrib_d]\n",
    "    net_distrib_d   = [duet_embed_d(d) for d in net_distrib_d]\n",
    "    net_distrib_d   = [C.reshape(d, (num_hidden_nodes * num_pooling_windows_doc, 1)) for d in net_distrib_d]\n",
    "\n",
    "    net_distrib     = [C.element_times(net_distrib_q, d) for d in net_distrib_d]\n",
    "    net_distrib     = [duet_distrib(d) for d in net_distrib]\n",
    "    net_distrib     = [C.reshape(d, (1, 1)) for d in net_distrib]\n",
    "    net_distrib     = C.splice(*net_distrib)\n",
    "                        \n",
    "    net             = C.plus(net_local, net_distrib)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate\n",
    "\n",
    "Next, we define the train and eval methods. This includes specifying the loss function for training and other hyper-parameters such as minibatch size and learning rate. Remember to try a few different scales for the learning rate to make sure it's not too large or too small (I typically run with 0.5, 0.1, 0.01, 0.001, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_file, ngraphs_file, num_docs, num_meta_cols):\n",
    "    \n",
    "    # initialize train data readers\n",
    "    reader_train = DataReader(train_file, ngraphs_file, num_docs, num_meta_cols, True)\n",
    "       \n",
    "    # input variables denoting the features and label data\n",
    "    features_local         = C.input_variable((reader_train.num_docs, reader_train.max_query_words, reader_train.max_doc_words), np.float32)\n",
    "    features_distrib_query = C.input_variable((reader_train.num_ngraphs, reader_train.max_query_words), np.float32)\n",
    "    features_distrib_docs  = C.input_variable((reader_train.num_docs, reader_train.num_ngraphs, reader_train.max_doc_words), np.float32)\n",
    "    labels                 = C.input_variable((reader_train.num_docs), np.float32)\n",
    "\n",
    "    # Instantiate the Duet neural document ranking model and specify loss function\n",
    "    z = duet(features_local, features_distrib_query, features_distrib_docs, reader_train.num_ngraphs, reader_train.max_query_words, reader_train.max_doc_words, reader_train.num_docs)\n",
    "    ce = C.cross_entropy_with_softmax(z, labels)\n",
    "    pe = C.classification_error(z, labels)\n",
    "\n",
    "    # Instantiate the trainer object to drive the model training\n",
    "    lr_per_minibatch = C.learning_rate_schedule(0.001, C.UnitType.minibatch)\n",
    "    progress_printers = [C.logging.ProgressPrinter(freq=100, tag='Training', gen_heartbeat=False)]\n",
    "    trainer = C.Trainer(z, (ce, pe), [C.sgd(z.parameters, lr=lr_per_minibatch)], progress_printers)\n",
    "\n",
    "    # Get minibatches of training data and perform model training\n",
    "    minibatch_size = 64\n",
    "    minibatches_per_epoch = 32\n",
    "    epochs = 4\n",
    "    \n",
    "    C.logging.log_number_of_parameters(ce)\n",
    "    print()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        for j in range(minibatches_per_epoch):\n",
    "            train_features_local, train_features_distrib_query, train_features_distrib_docs, train_labels, train_meta, actual_mb_size = reader_train.get_minibatch(minibatch_size)\n",
    "            trainer.train_minibatch({features_local : train_features_local, features_distrib_query : train_features_distrib_query, features_distrib_docs : train_features_distrib_docs, labels : train_labels})\n",
    "        trainer.summarize_training_progress()\n",
    "\n",
    "    return z\n",
    "\n",
    "def eval(model, test_file, ngraphs_file, num_docs, num_meta_cols, score_file):\n",
    "    \n",
    "    minibatch_size = 64\n",
    "    actual_mb_size = minibatch_size\n",
    "    \n",
    "    # initialize test data readers\n",
    "    reader_test  = DataReader(test_file, ngraphs_file, num_docs, num_meta_cols, False)\n",
    "\n",
    "    with open(score_file, mode='w') as f:\n",
    "        while(actual_mb_size == minibatch_size):\n",
    "            test_features_local, test_features_distrib_query, test_features_distrib_docs, test_labels, test_meta, actual_mb_size = reader_test.get_minibatch(minibatch_size)\n",
    "            if actual_mb_size > 0:\n",
    "                result = model.eval({model.arguments[0] : test_features_local, model.arguments[1] : test_features_distrib_query, model.arguments[2] : test_features_distrib_docs})\n",
    "                result = result.reshape((actual_mb_size, num_docs))\n",
    "                result = [row[0] for row in result]\n",
    "                for idx in range(actual_mb_size):\n",
    "                    f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(test_meta[idx][0], test_meta[idx][1], test_meta[idx][2], result[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute metric\n",
    "\n",
    "After we run the eval step to score all the query-document pairs in the test data we need to compute our metric-of-choice over the eval output. We use NDCG for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ComputeDCG(sorted_ranks):\n",
    "    dcg = 0\n",
    "    for pos, rating in enumerate(sorted_ranks):\n",
    "        dcg += ((2^rating - 1)/math.log2(pos + 2))\n",
    "    return dcg\n",
    "\n",
    "def ComputeNDCGPerQuery(ideal_ratings, scored_ratings):\n",
    "    ideal_ranks = sorted(ideal_ratings, reverse=True)\n",
    "    model_ranks = [pair[1] for pair in sorted(scored_ratings, key=lambda tup: tup[0], reverse=True)]\n",
    "    ideal_dcg = ComputeDCG(ideal_ranks)\n",
    "    model_dcg = ComputeDCG(model_ranks)\n",
    "    return model_dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "\n",
    "def ComputeNDCG(score_file, ndcg_pos):\n",
    "    ndcg = 0\n",
    "    curr_qid = -1\n",
    "    ideal_ratings = []\n",
    "    scored_ratings = []\n",
    "    q_count = 0\n",
    "    \n",
    "    with open(score_file, mode='r') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            q_id   = row[0]\n",
    "            doc_id = row[1]\n",
    "            rating = int(row[1])\n",
    "            score  = float(row[1])\n",
    "            \n",
    "            if q_id != curr_qid:\n",
    "                ndcg += ComputeNDCGPerQuery(ideal_ratings, scored_ratings)\n",
    "                q_count += 1\n",
    "                ideal_ratings = []\n",
    "                scored_ratings = []\n",
    "                \n",
    "            curr_qid = q_id\n",
    "            ideal_ratings.append(rating)\n",
    "            scored_ratings.append((score, rating))\n",
    "            \n",
    "    ndcg += ComputeNDCGPerQuery(ideal_ratings, scored_ratings)\n",
    "    q_count += 1\n",
    "    \n",
    "    return ndcg / q_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run\n",
    "Finally, we are ready to put all the pieces of the puzzle together and train and evaluate our Duet model. Woohoo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngraphs_file = \"data\\\\ngraphs.txt\"\n",
    "train_file = \"data\\\\train.txt\"\n",
    "test_file = \"data\\\\test.txt\"\n",
    "score_file = \"data\\\\score.txt\"\n",
    "\n",
    "model = train(train_file, ngraphs_file, 2, 0)\n",
    "eval(model, test_file, ngraphs_file, 2, 3, score_file)\n",
    "ndcg = ComputeNDCG(score_file, 10)\n",
    "print(\"test ndcg = {}\".format(ndcg))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [cntk-py34]",
   "language": "python",
   "name": "Python [cntk-py34]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
